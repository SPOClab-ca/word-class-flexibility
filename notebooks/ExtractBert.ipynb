{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract BERT\n",
    "\n",
    "Notebook to experiment with libraries to extract parts of BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1103 01:00:27.026603 140463182931712 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1103 01:00:28.313264 140463182931712 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = transformers.BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bert_input(sentences):\n",
    "  def pad_to_length(tokens, desired_len):\n",
    "    return tokens + (['[PAD]'] * (desired_len - len(tokens)))\n",
    "  bert_tokens = [bert_tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "  max_len = max([len(tokens) for tokens in bert_tokens])\n",
    "  padded_tokens = [pad_to_length(tokens, max_len) for tokens in bert_tokens]\n",
    "  padded_ids = [bert_tokenizer.encode(tokens) for tokens in padded_tokens]\n",
    "  attn_mask = [[1 if token != '[PAD]' else 0 for token in tokens] for tokens in padded_tokens]\n",
    "  return padded_tokens, padded_ids, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'cat', 'is', 'called', 'xiao', '##nu', '##an', '##hu', '##o', 'and', 'she', 'is', 'warm', 'and', 'fluffy'], ['this', 'is', 'a', 'much', 'shorter', 'sentence', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
      "[[2026, 4937, 2003, 2170, 19523, 11231, 2319, 6979, 2080, 1998, 2016, 2003, 4010, 1998, 27036], [2023, 2003, 1037, 2172, 7820, 6251, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "STR1 = \"My cat is called Xiaonuanhuo and she is warm and fluffy\"\n",
    "STR2 = \"this is a much shorter sentence\"\n",
    "padded_tokens, padded_ids, attn_mask = convert_to_bert_input([STR1, STR2])\n",
    "print(padded_tokens)\n",
    "print(padded_ids)\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4095,  0.0730,  0.0156,  ...,  0.0193,  0.1457,  0.6264],\n",
       "         [-0.6220, -0.3098,  0.4594,  ..., -0.2029,  0.3284,  1.0646],\n",
       "         [-1.0167, -0.3767,  0.4307,  ..., -0.0539,  0.4236,  0.9048],\n",
       "         ...,\n",
       "         [-0.7988,  0.2165,  0.7978,  ..., -0.0076,  0.0286,  0.4517],\n",
       "         [-0.7674, -0.0174,  0.5640,  ...,  0.0461,  0.0452,  0.5110],\n",
       "         [-0.7671,  0.1120,  0.8036,  ...,  0.1290,  0.1001,  0.5393]],\n",
       "\n",
       "        [[-0.2225,  0.0793, -0.2679,  ...,  0.0747,  0.3448,  0.2930],\n",
       "         [-0.3072, -0.0128, -0.4142,  ...,  0.2680,  0.3103,  0.5950],\n",
       "         [-0.6995,  0.0327, -0.3212,  ...,  0.5684,  0.3032,  0.5805],\n",
       "         ...,\n",
       "         [-0.2036,  0.1585, -0.1651,  ...,  0.0816,  0.2754,  0.1891],\n",
       "         [-0.2093,  0.1336, -0.1721,  ...,  0.0810,  0.2852,  0.1886],\n",
       "         [-0.1874,  0.1357, -0.1805,  ...,  0.0776,  0.2798,  0.1795]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings = bert_model(torch.tensor(padded_ids), attention_mask=torch.tensor(attn_mask))\n",
    "bert_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final hidden layer, one for each token\n",
    "bert_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pooled layer\n",
    "bert_embeddings[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nth hidden layer of BERT\n",
    "layer = 7\n",
    "bert_embeddings[2][layer].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
