{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo Contextual Embeddings\n",
    "\n",
    "In this notebook, we use contextual embeddings from ELMo to study semantic change of conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import allennlp.commands.elmo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "import src.corpus\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CoNLL-U file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TokenList<Al, -, Zaman, :, American, forces, killed, Shaikh, Abdullah, al, -, Ani, ,, the, preacher, at, the, mosque, in, the, town, of, Qaim, ,, near, the, Syrian, border, .>,\n",
       " TokenList<[, This, killing, of, a, respected, cleric, will, be, causing, us, trouble, for, years, to, come, ., ]>,\n",
       " TokenList<DPA, :, Iraqi, authorities, announced, that, they, had, busted, up, 3, terrorist, cells, operating, in, Baghdad, .>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UD_FILE = \"../data/en_ewt-ud-train.conllu\"\n",
    "\n",
    "ud = src.corpus.POSCorpus.create_from_ud(data_file_path=UD_FILE)\n",
    "ud.data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ELMo on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:29<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "elmo = allennlp.commands.elmo.ElmoEmbedder(cuda_device=0)\n",
    "data_as_tokens = [[t['word'] for t in sentence] for sentence in ud.sentences]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "elmo_embeddings = []\n",
    "for ix in tqdm.tqdm(range(0, len(data_as_tokens), BATCH_SIZE)):\n",
    "  batch = data_as_tokens[ix : ix+BATCH_SIZE]\n",
    "  batch_embeddings = elmo.embed_batch(batch)\n",
    "  # Only take embeddings from last ELMo layer\n",
    "  batch_embeddings = [x[-1] for x in batch_embeddings]\n",
    "  elmo_embeddings.extend(batch_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo embeddings of instances of a fixed lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings_for_lemma(lemma):\n",
    "  noun_embeddings = []\n",
    "  verb_embeddings = []\n",
    "\n",
    "  for sentence_ix in range(len(ud.data)):\n",
    "    token_list = ud.data[sentence_ix]\n",
    "    embeddings = elmo_embeddings[sentence_ix]\n",
    "    for i in range(len(token_list)):\n",
    "      if token_list[i]['lemma'] == lemma:\n",
    "        if token_list[i]['upostag'] == 'NOUN':\n",
    "          noun_embeddings.append(embeddings[i])\n",
    "        elif token_list[i]['upostag'] == 'VERB':\n",
    "          verb_embeddings.append(embeddings[i])\n",
    "\n",
    "  noun_embeddings = np.vstack(noun_embeddings)\n",
    "  verb_embeddings = np.vstack(verb_embeddings)\n",
    "  return noun_embeddings, verb_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_LEMMA = \"work\"\n",
    "noun_embeddings, verb_embeddings = get_elmo_embeddings_for_lemma(FIXED_LEMMA)\n",
    "print(\"Noun instances:\", noun_embeddings.shape[0])\n",
    "print(\"Verb instances:\", verb_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "all_embeddings = pca.fit_transform(np.vstack([noun_embeddings, verb_embeddings]))\n",
    "all_embeddings_df = pd.DataFrame({'x0': all_embeddings[:,0], 'x1': all_embeddings[:,1]})\n",
    "all_embeddings_df['pos'] = ['noun'] * len(noun_embeddings) + ['verb'] * len(verb_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.scatterplot(data=all_embeddings_df, x='x0', y='x1', hue='pos')\n",
    "plot.set(title=\"ELMo embeddings for lemma: '%s'\" % FIXED_LEMMA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity between noun and verb usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_count_df = ud.get_per_lemma_stats()\n",
    "\n",
    "# Filter: must have at least [x] noun and [x] verb usages\n",
    "lemma_count_df = lemma_count_df[(lemma_count_df['noun_count'] >= 10) & (lemma_count_df['verb_count'] >= 10)]\n",
    "lemma_count_df = lemma_count_df.sort_values('total_count', ascending=False)\n",
    "print('Remaining lemmas:', len(lemma_count_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nv_cosine_similarity(row):\n",
    "  noun_embeddings, verb_embeddings = get_elmo_embeddings_for_lemma(row.lemma)\n",
    "  \n",
    "  avg_noun_embedding = np.mean(noun_embeddings, axis=0)\n",
    "  avg_verb_embedding = np.mean(verb_embeddings, axis=0)\n",
    "\n",
    "  return float(sklearn.metrics.pairwise.cosine_similarity(avg_noun_embedding[np.newaxis,:], avg_verb_embedding[np.newaxis,:]))\n",
    "\n",
    "lemma_count_df['nv_cosine_similarity'] = lemma_count_df.apply(get_nv_cosine_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_count_df[['lemma', 'noun_count', 'verb_count', 'majority_tag', 'nv_cosine_similarity']] \\\n",
    "  .sort_values('nv_cosine_similarity').head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_count_df[['lemma', 'noun_count', 'verb_count', 'majority_tag', 'nv_cosine_similarity']] \\\n",
    "  .sort_values('nv_cosine_similarity', ascending=False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.distplot(lemma_count_df[lemma_count_df.majority_tag == 'NOUN'].nv_cosine_similarity, label='Base=N')\n",
    "plot = sns.distplot(lemma_count_df[lemma_count_df.majority_tag == 'VERB'].nv_cosine_similarity, label='Base=V')\n",
    "plt.legend()\n",
    "plot.set(title=\"Average Cosine Similarity between Noun/Verb Usage\",\n",
    "         xlabel=\"Cosine Similarity\", ylabel=\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test of difference in mean\n",
    "import scipy.stats\n",
    "scipy.stats.ttest_ind(lemma_count_df[lemma_count_df.majority_tag == 'NOUN'].nv_cosine_similarity,\n",
    "                      lemma_count_df[lemma_count_df.majority_tag == 'VERB'].nv_cosine_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
